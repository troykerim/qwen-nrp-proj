model_name_or_path: /workspace/models/Qwen2.5-VL-7B-Instruct
template: qwen
trust_remote_code: true
cache_dir: /workspace/.cache/huggingface

stage: sft
do_train: true

finetuning_type: qlora

# QLoRA 4-bit quantization
quantization_bit: 4
quantization_method: bitsandbytes

resize_vocab: false

dataset: maica_qwen_sft
dataset_dir: /workspace/data
media_dir: /workspace/data/images
packing: false

# QLoRA adapter settings (NOT LoRA)
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target: "q_proj,k_proj,v_proj,o_proj"

# Training hyperparameters 
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 2e-4
num_train_epochs: 1
lr_scheduler_type: cosine
warmup_ratio: 0.05
weight_decay: 0.0

gradient_checkpointing: true
fp16: false
bf16: true
dataloader_num_workers: 0

freeze_vision_tower: false
freeze_multi_modal_projector: false

logging_steps: 10
logging_first_step: true
save_strategy: steps
save_steps: 200
save_total_limit: 3

output_dir: /workspace/output/qwen_qlora
overwrite_output_dir: true
save_safetensors: true
report_to: none
