roy@troy-yoga:~$ kubectl logs -f job/qwen-unsloth-job -n nsf-maica
Found 2 pods, using pod/qwen-unsloth-job-m2nvs
+ python -V
Python 3.10.12
+ nvidia-smi
Tue Dec 16 04:50:29 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A10                     On  |   00000000:C1:00.0 Off |                    0 |
|  0%   36C    P8             24W /  150W |       3MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
+ echo '[INFO] Checking Qwen model in PVC...'
+ ls -al /workspace/models/Qwen2.5-VL-7B-Instruct
[INFO] Checking Qwen model in PVC...
total 16207052
drwxr-xr-x 2 root root       4096 Nov  8 22:11 .
drwxr-xr-x 4 root root         54 Nov  8 21:48 ..
-rw-rw-r-- 1 1000 1000      18574 Nov  8 21:57 README.md
-rw-rw-r-- 1 1000 1000       1050 Nov  8 21:57 chat_template.json
-rw-rw-r-- 1 1000 1000       1374 Nov  8 21:57 config.json
-rw-rw-r-- 1 1000 1000        216 Nov  8 21:57 generation_config.json
-rw-rw-r-- 1 1000 1000    1671839 Nov  8 21:57 merges.txt
-rw-rw-r-- 1 1000 1000 3900233256 Nov  8 21:58 model-00001-of-00005.safetensors
-rw-rw-r-- 1 1000 1000 3864726320 Nov  8 22:00 model-00002-of-00005.safetensors
-rw-rw-r-- 1 1000 1000 3864726424 Nov  8 22:01 model-00003-of-00005.safetensors
-rw-rw-r-- 1 1000 1000 3864733680 Nov  8 22:03 model-00004-of-00005.safetensors
-rw-rw-r-- 1 1000 1000 1089994880 Nov  8 22:03 model-00005-of-00005.safetensors
-rw-rw-r-- 1 1000 1000      57619 Nov  8 22:03 model.safetensors.index.json
-rw-rw-r-- 1 1000 1000        350 Nov  8 22:03 preprocessor_config.json
-rw-rw-r-- 1 1000 1000    7031645 Nov  8 22:03 tokenizer.json
-rw-rw-r-- 1 1000 1000       5702 Nov  8 22:03 tokenizer_config.json
-rw-rw-r-- 1 1000 1000    2776833 Nov  8 22:03 vocab.json
[INFO] Dataset contents:
+ echo '[INFO] Dataset contents:'
+ ls -al /workspace/data
total 3888
drwxr-xr-x  6 root root     169 Nov 28 18:04 .
drwxr-xr-x 12 root root    4096 Dec 16 04:42 ..
drwxr-xr-x  4 root root      32 Nov  9 23:28 dataset
drwxr-xr-x  2 root root   86016 Oct 13 18:51 images
drwxr-xr-x  2 root root   86016 Oct 13 18:51 labels
-rw-rw-r--  1 1000 1000     230 Oct 13 23:12 maica_internvl_sft.json
-rw-rw-r--  1 1000 1000 1622935 Oct 13 23:12 maica_internvl_sft.jsonl
drwxr-xr-x  4 root root      34 Oct 29 04:25 test_images
-rw-rw-r--  1 1000 1000 1903572 Nov 28 18:04 train.jsonl
-rw-rw-r--  1 1000 1000  214546 Nov 28 18:04 valid.jsonl
[INFO] Starting QLoRA fine-tuning...
+ echo '[INFO] Starting QLoRA fine-tuning...'
+ python /workspace/qwen-train-unsloth.py
/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
Path check:
Model: True
Dataset: True
Train JSONL: True
Valid JSONL: True
==((====))==  Unsloth 2025.12.5: Fast Qwen2_5_Vl patching. Transformers: 4.57.1.
   \\   /|    NVIDIA A10. Num GPUs = 1. Max memory: 22.058 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [1:21:33<00:00, 978.74s/it] 
The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.
Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.
The model is already on multiple devices. Skipping the move to device specified in `args`.
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,794 | Num Epochs = 3 | Total steps = 675
O^O/ \_/ \    Batch size per device = 1 | Gradient accumulation steps = 8
\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8
 "-____-"     Trainable parameters = 51,521,536 of 8,343,688,192 (0.62% trained)
Train samples: 1794
Valid samples: 186
Starting training...

  0%|          | 0/675 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/workspace/qwen-train-unsloth.py", line 184, in <module>
    trainer.train()
  File "/workspace/unsloth_compiled_cache/UnslothSFTTrainer.py", line 55, in wrapper
    output = f(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
  File "<string>", line 272, in _fast_inner_training_loop
  File "/usr/local/lib/python3.10/dist-packages/unsloth_zoo/loss_utils.py", line 326, in _unsloth_get_batch_samples
    batch_samples += [next(epoch_iterator)]
  File "/usr/local/lib/python3.10/dist-packages/accelerate/data_loader.py", line 567, in __iter__
    current_batch = next(dataloader_iter)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py", line 732, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py", line 788, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py", line 45, in __call__
    return self.torch_call(features)
  File "/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py", line 1033, in torch_call
    batch = pad_without_fast_tokenizer_warning(
  File "/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py", line 66, in pad_without_fast_tokenizer_warning
    padded = tokenizer.pad(*pad_args, **pad_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py", line 3374, in pad
    raise ValueError(
ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['messages']

