{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4611cf24",
   "metadata": {},
   "source": [
    "# Juypter Notebook used in Google Colab \n",
    "This is notebook contains cells that used to test the exact py script that is used in Nautilus.  Since Nautilus does not have a simple way to do debugging, it is better to use a different service to do the debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194b7735",
   "metadata": {},
   "source": [
    "Import Libraries into Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cea4171",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -U bitsandbytes\n",
    "!pip install torch\n",
    "#!pip install peft\n",
    "#!pip install transformers\n",
    "!pip install qwen-vl-utils\n",
    "!pip install -q \"transformers==4.57.1\" \"accelerate>=0.34.2\" \"peft>=0.13.2\" \"bitsandbytes>=0.43.0\" \"sentencepiece\" \"einops\" \"timm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488aac2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0d163d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import json\n",
    "# import os\n",
    "\n",
    "MODEL_PATH = \"/content/drive/MyDrive/VLM_MODELS/Qwen2.5-VL-7B-Instruct\"\n",
    "DATASET_DIR = \"/content/drive/MyDrive/model_datasets/dataset2\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/output/waste_detection\"\n",
    "TRAINING_DATA = \"/content/drive/MyDrive/model_datasets/train2.jsonl\"\n",
    "VAL_DATA = \"/content/drive/MyDrive/model_datasets/valid2.jsonl\"\n",
    "\n",
    "\n",
    "print(\"Checking paths...\\n\")\n",
    "print(\"Model path exists:\", os.path.exists(MODEL_PATH))\n",
    "print(\"Training data exists:\", os.path.exists(TRAINING_DATA))\n",
    "print(\"Validation data exists:\", os.path.exists(VAL_DATA))\n",
    "print(\"Output folder exists:\", os.path.exists(OUTPUT_DIR))\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    print(\"Output directory created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595a36f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "class WasteDetectionDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom dataset that doesn't use Arrow/Datasets library\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"messages\": self.data[idx][\"messages\"]}\n",
    "\n",
    "class WasteDetectionDataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, features):\n",
    "        texts, batch_images, batch_videos = [], [], []\n",
    "\n",
    "        for f in features:\n",
    "            msgs = f[\"messages\"]\n",
    "            # Apply chat template\n",
    "            text = self.processor.apply_chat_template(\n",
    "                msgs, tokenize=False, add_generation_prompt=False\n",
    "            )\n",
    "            texts.append(text)\n",
    "\n",
    "            imgs, vids = process_vision_info(msgs)\n",
    "            batch_images.append(imgs or [])\n",
    "            batch_videos.append(vids or [])\n",
    "\n",
    "        images = None if all(len(x) == 0 for x in batch_images) else batch_images\n",
    "        videos = None if all(len(x) == 0 for x in batch_videos) else batch_videos\n",
    "\n",
    "        batch = self.processor(\n",
    "            text=texts,\n",
    "            images=images,\n",
    "            max_length=2048,   #None, It was either 512 or was too large?\n",
    "            videos=videos,\n",
    "            padding=True, # padding=True, truncation=True,\n",
    "            truncation = False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Get labels from input_ids\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "        # For Qwen models, we need to mask everything except assistant response\n",
    "        # The key is to find where assistant response tokens start\n",
    "        for idx in range(len(features)):\n",
    "            # Get the full sequence\n",
    "            input_ids_list = batch[\"input_ids\"][idx].tolist()\n",
    "\n",
    "            # Mask everything first\n",
    "            labels[idx] = -100\n",
    "\n",
    "            # Find assistant content from original messages\n",
    "            msgs = features[idx][\"messages\"]\n",
    "            assistant_msg = None\n",
    "            for msg in msgs:\n",
    "                if msg[\"role\"] == \"assistant\":\n",
    "                    assistant_msg = msg[\"content\"]\n",
    "                    break\n",
    "\n",
    "            if assistant_msg:\n",
    "                # Tokenize just the assistant response\n",
    "                asst_tokens = self.processor.tokenizer.encode(\n",
    "                    assistant_msg,\n",
    "                    add_special_tokens=False\n",
    "                )\n",
    "\n",
    "                # Find these tokens in the full sequence (from the end)\n",
    "                seq_len = (batch[\"attention_mask\"][idx] == 1).sum().item()\n",
    "\n",
    "                # Match from the end backwards\n",
    "                asst_len = len(asst_tokens)\n",
    "                if asst_len > 0 and asst_len < seq_len:\n",
    "                    # Check if the tokens match at the end\n",
    "                    expected_start = seq_len - asst_len\n",
    "\n",
    "                    # Unmask the assistant portion\n",
    "                    labels[idx, expected_start:seq_len] = batch[\"input_ids\"][idx, expected_start:seq_len]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "print(\"Collator defined - matching from end of sequence\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ff0a7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# QLoRA 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model with 8-bit quantization\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# Load processor\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "print(f\"Model loaded with 8-bit quantization: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff3ab30",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "model.enable_input_require_grads()\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(\"8-bit QLoRA configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfcf392",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# Load training data\n",
    "print(\"Loading training data...\")\n",
    "train_data = []\n",
    "with open(TRAINING_DATA, 'r') as f:\n",
    "    for line in f:\n",
    "        train_data.append(json.loads(line))\n",
    "\n",
    "train_dataset = WasteDetectionDataset(train_data)\n",
    "print(f\"Loaded {len(train_dataset)} training samples\")\n",
    "\n",
    "# Load validation data\n",
    "print(\"Loading validation data...\")\n",
    "val_data = []\n",
    "#with open(VALIDATION_DATA, 'r') as f:\n",
    "with open(VAL_DATA, 'r') as f:\n",
    "    for line in f:\n",
    "        val_data.append(json.loads(line))\n",
    "\n",
    "val_dataset = WasteDetectionDataset(val_data)\n",
    "print(f\"Loaded {len(val_dataset)} validation samples\")\n",
    "\n",
    "# Load test data (for later evaluation)\n",
    "#print(\"Loading test data...\")\n",
    "#test_data = []\n",
    "#with open(TEST_DATA, 'r') as f:\n",
    " #   for line in f:\n",
    " #       test_data.append(json.loads(line))\n",
    "\n",
    "# test_dataset = WasteDetectionDataset(test_data)\n",
    "# print(f\"✓ Loaded {len(test_dataset)} test samples\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Dataset Summary:\")\n",
    "print(f\"  Train:      {len(train_dataset)} examples\")\n",
    "print(f\"  Validation: {len(val_dataset)} examples\")\n",
    "# print(f\"  Test:       {len(test_dataset)} examples\")\n",
    "# print(f\"  Total:      {len(train_dataset) + len(val_dataset) + len(test_dataset)} examples\")\n",
    "print(f\"  Total:      {len(train_dataset) + len(val_dataset)} examples\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41c27e7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Configuring TrainingArguments...\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,  # originally 5\n",
    "\n",
    "    # Batch sizes (for 8-bit LoRA), DO NOT USE WITH L4!\n",
    "    # per_device_train_batch_size=2,\n",
    "    # per_device_eval_batch_size=2,\n",
    "    # gradient_accumulation_steps=4,\n",
    "\n",
    "    # Sizes for QLoRA\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "\n",
    "    # Learning rate\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=100,\n",
    "\n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "\n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "\n",
    "    # Saving\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "\n",
    "    # Optimization\n",
    "    gradient_checkpointing=False,   # Was original set to true, but was sending a Warning in the last cell\n",
    "    # optim=\"adamw_8bit\",  # 8-bit optimizer, use with LoRA\n",
    "    optim=\"adamw_torch\",  # Should be ok with QLoRA?\n",
    "\n",
    "    # Memory\n",
    "    dataloader_pin_memory=True,\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "    # Precision\n",
    "    bf16=(torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8),\n",
    "    fp16=not (torch.cuda.get_device_capability()[0] >= 8),\n",
    "\n",
    "    # Reporting\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"TrainingArguments ready.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e1548e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 9\n",
    "print(\"Initializing Trainer...\")\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "data_collator = WasteDetectionDataCollator(processor)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "print(\"Trainer initialized.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e76e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL COLLATOR VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test two samples\n",
    "test_batch = data_collator([train_dataset[0], train_dataset[1]])\n",
    "\n",
    "print(f\"\\nBatch size: {test_batch['input_ids'].shape[0]}\")\n",
    "print(f\"Sequence length: {test_batch['input_ids'].shape[1]}\")\n",
    "\n",
    "for i in range(test_batch['input_ids'].shape[0]):\n",
    "    total = test_batch[\"labels\"][i].numel()\n",
    "    non_masked = (test_batch[\"labels\"][i] != -100).sum().item()\n",
    "\n",
    "    msgs = [train_dataset[0], train_dataset[1]][i][\"messages\"]\n",
    "    expected = 0\n",
    "    for msg in msgs:\n",
    "        if msg[\"role\"] == \"assistant\":\n",
    "            expected = len(processor.tokenizer.encode(\n",
    "                msg[\"content\"], add_special_tokens=False\n",
    "            ))\n",
    "            break\n",
    "\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Learning from: {non_masked}/{total} tokens ({non_masked/total*100:.1f}%)\")\n",
    "    print(f\"  Expected: {expected} tokens\")\n",
    "    print(f\"  Match: {'✓ PERFECT' if abs(expected - non_masked) < 5 else '✗ MISMATCH'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COLLATOR STATUS: READY FOR TRAINING ✓\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7ccb51",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Training on {len(train_dataset)} examples\")\n",
    "print(f\"Validating on {len(val_dataset)} examples\")\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "trainer.train()\n",
    "print(\"\\nSaving model and processor...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Training complete! Model saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e34bcd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# REVISE\n",
    "\n",
    "by_step = defaultdict(dict)\n",
    "for log in trainer.state.log_history:\n",
    "    step = log.get(\"step\")\n",
    "    if step is None:\n",
    "        continue\n",
    "    for k, v in log.items():\n",
    "        if k == \"step\":\n",
    "            continue\n",
    "        by_step[step][k] = v\n",
    "\n",
    "print(\"Step\\tTrain Loss\\tEval Loss\")\n",
    "for step in sorted(by_step.keys()):\n",
    "    train_loss = by_step[step].get(\"loss\")\n",
    "    eval_loss = by_step[step].get(\"eval_loss\")\n",
    "    if train_loss is None and eval_loss is None:\n",
    "        continue\n",
    "    print(f\"{step}\\t{train_loss}\\t{eval_loss}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
