apiVersion: batch/v1
kind: Job
metadata:
  name: qwen-unsloth-job
  namespace: nsf-maica
spec:
  backoffLimit: 0
  ttlSecondsAfterFinished: 86400
  template:
    spec:
      restartPolicy: Never

      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                  - NVIDIA-A10
                  # - Tesla-V100-SXM2-32GB

      containers:
      - name: train
        image: docker.io/troyk500/qwen-nrp-unsloth:v1
        imagePullPolicy: Always
        command: ["/bin/bash", "-lc"]
        args:
          - |
            set -euxo pipefail
            python -V ; nvidia-smi || true

            echo "[INFO] Checking Qwen model in PVC..."
            ls -al /workspace/models/Qwen2.5-VL-7B-Instruct || true

            echo "[INFO] Dataset contents:"
            ls -al /workspace/data || true

            echo "[INFO] Starting QLoRA fine-tuning..."
            python /workspace/qwen-train-unsloth.py

        env:
        - name: DISABLE_VERSION_CHECK
          value: "1"
        - name: HF_HOME
          value: /workspace/.cache/huggingface
        - name: TRANSFORMERS_CACHE
          value: /workspace/.cache/huggingface
        - name: HF_DATASETS_CACHE
          value: /workspace/.cache/huggingface/datasets
        - name: PIP_CACHE_DIR
          value: /workspace/.cache/pip
        - name: TMPDIR
          value: /workspace/tmp
        - name: HF_HUB_ENABLE_HF_TRANSFER
          value: "1"
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-token
              key: HUGGING_FACE_HUB_TOKEN
        - name: PYTORCH_CUDA_ALLOC_CONF
          value: "max_split_size_mb:64,expandable_segments:False"
        - name: TOKENIZERS_PARALLELISM
          value: "false"
        - name: OMP_NUM_THREADS
          value: "1"
        - name: MKL_NUM_THREADS
          value: "1"

        resources:
          requests:
            nvidia.com/gpu: 1
            cpu: "10"
            memory: "30Gi"
            ephemeral-storage: "60Gi"
          limits:
            nvidia.com/gpu: 1
            cpu: "16"
            memory: "64Gi"
            ephemeral-storage: "100Gi"

        volumeMounts:
        - name: work
          mountPath: /workspace

      volumes:
      - name: work
        persistentVolumeClaim:
          claimName: internvl-work
