

# UPDATE THE FILE PATHS

apiVersion: batch/v1
kind: Job
metadata:
  name: qwen-test-job
  namespace: nsf-maica
spec:
  backoffLimit: 0
  ttlSecondsAfterFinished: 86400
  template:
    spec:
      restartPolicy: Never

      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                  - NVIDIA-A10

      containers:
      - name: inference
        image: docker.io/troyk500/qwen-nrp-unsloth:v1
        imagePullPolicy: Always
        command: ["/bin/bash", "-lc"]
        args:
          - |
            set -euxo pipefail
            python -V
            nvidia-smi || true

            echo "[INFO] Checking adapter output directory..."
            ls -al /workspace/output/qwen_unsloth4

            echo "[INFO] Checking test dataset..."
            ls -al /workspace/data/jam-causing-material/test
            ls -al /workspace/data/test.jsonl

            echo "[INFO] Running inference..."
            python /workspace/qwen-test.py

        env:
        - name: DISABLE_VERSION_CHECK
          value: "1"

        - name: HF_HOME
          value: /workspace/.cache/huggingface
        - name: TRANSFORMERS_CACHE
          value: /workspace/.cache/huggingface
        - name: HF_DATASETS_CACHE
          value: /workspace/.cache/huggingface/datasets
        - name: PIP_CACHE_DIR
          value: /workspace/.cache/pip
        - name: TMPDIR
          value: /workspace/tmp

        - name: HF_HUB_ENABLE_HF_TRANSFER
          value: "1"

        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-token
              key: HUGGING_FACE_HUB_TOKEN

        - name: PYTORCH_CUDA_ALLOC_CONF
          value: "max_split_size_mb:64,expandable_segments:False"

        - name: TOKENIZERS_PARALLELISM
          value: "false"

        - name: OMP_NUM_THREADS
          value: "1"
        - name: MKL_NUM_THREADS
          value: "1"

        resources:
          requests:
            nvidia.com/gpu: 1
            cpu: "8"
            memory: "24Gi"
            ephemeral-storage: "40Gi"
          limits:
            nvidia.com/gpu: 1
            cpu: "16"
            memory: "64Gi"
            ephemeral-storage: "100Gi"

        volumeMounts:
        - name: work
          mountPath: /workspace

      volumes:
      - name: work
        persistentVolumeClaim:
          claimName: internvl-work

